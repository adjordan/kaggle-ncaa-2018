{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "root = \"/home/austin/Github/kaggle-ncaa-2018/\"\n",
    "\n",
    "data = pd.read_csv(root + \"derived_data/Master.csv\")\n",
    "matchups = pd.read_csv(\"/home/austin/Github/DataFiles/TourneyWinratesBySeed.csv\")\n",
    "tour_ratios = pd.read_csv(root + \"derived_data/ratios/NCAATourneyDetailedResultsRatios.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = range(2003, 2018)\n",
    "\n",
    "all_columns = data.columns.tolist()\n",
    "non_stats_columns = [\"Season\", \"TeamID\", \"Seed\", \"Elo\"]#, \"BPI\", \"Predictor_Score\"]\n",
    "stats_columns = [c for c in all_columns if c not in non_stats_columns]\n",
    "\n",
    "# stats_columns_A = [c+\"num\" for c in stats_columns]\n",
    "# stats_columns_B = [c+\"den\" for c in stats_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all data in a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_games = tour_ratios.loc[tour_ratios[\"Season\"].isin(seasons)]\n",
    "tourney_games = tourney_games.reset_index(drop=True)\n",
    "extra_cols = [\"NumTeamSeed\", \"DenTeamSeed\", \"NumTeamElo\", \"DenTeamElo\", \"HistWinPct\"]\n",
    "\n",
    "pred_data = pd.DataFrame(index=range(tourney_games.shape[0]), columns= [\"Season\"] + stats_columns + extra_cols + [\"NumTeamWon\"])\n",
    "pred_data.loc[:,\"NumTeamWon\"] = np.ones(tourney_games.shape[0])\n",
    "index = 0\n",
    "for i in range(len(seasons)):\n",
    "    season = seasons[i]\n",
    "    tourney_games_for_season = tourney_games.loc[tourney_games[\"Season\"] == season].reset_index()\n",
    "    for j, row in tourney_games_for_season.iterrows():\n",
    "        pred_data.loc[index, \"Season\"] = season\n",
    "        teamA_id = tourney_games_for_season.at[j, \"WTeamID\"]\n",
    "        teamB_id = tourney_games_for_season.at[j, \"LTeamID\"]\n",
    "        \n",
    "        teamA_seed = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Seed\"]\n",
    "        teamB_seed = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Seed\"]\n",
    "        \n",
    "        teamA_elo = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Elo\"]\n",
    "        teamB_elo = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Elo\"]\n",
    "        \n",
    "        teamA_stats = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season), stats_columns].as_matrix()\n",
    "        teamB_stats = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season), stats_columns].as_matrix()\n",
    "        \n",
    "        r = random.random()\n",
    "        \n",
    "        if r > 0.5:\n",
    "            pred_data.at[index, stats_columns] = (teamA_stats / teamB_stats).ravel()\n",
    "            pred_data.at[index, \"NumTeamSeed\"] = teamA_seed\n",
    "            pred_data.at[index, \"DenTeamSeed\"] = teamB_seed\n",
    "            pred_data.at[index, \"NumTeamElo\"] = teamA_elo\n",
    "            pred_data.at[index, \"DenTeamElo\"] = teamB_elo\n",
    "            pred_data.at[index, \"HistWinPct\"] = matchups[(matchups[\"WinSeed\"] == teamA_seed) & (matchups[\"LoseSeed\"] == teamB_seed)][\"1985\"]\n",
    "        else:\n",
    "            pred_data.at[index, stats_columns] = (teamB_stats / teamA_stats).ravel()\n",
    "            pred_data.at[index, \"NumTeamSeed\"] = teamB_seed\n",
    "            pred_data.at[index, \"DenTeamSeed\"] = teamA_seed\n",
    "            pred_data.at[index, \"NumTeamElo\"] = teamB_elo\n",
    "            pred_data.at[index, \"DenTeamElo\"] = teamA_elo\n",
    "            pred_data.at[index, \"HistWinPct\"] = matchups[(matchups[\"WinSeed\"] == teamB_seed) & (matchups[\"LoseSeed\"] == teamA_seed)][\"1985\"]\n",
    "            pred_data.at[index, \"NumTeamWon\"] = 0\n",
    "        \n",
    "        index += 1\n",
    "\n",
    "pred_data = pred_data.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select which stats to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FGM', 'FGA', 'FGM3', 'FGA3', 'FTM', 'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', 'PIE', 'FG_PCT', 'TURNOVER_RATE', 'OFF_REB_PCT', 'FT_RATE', '4FACTOR', 'OFF_EFF', 'DEF_EFF', 'ASSIST_RATIO', 'DEF_REB_PCT', 'FT_PCT', 'WINPCT', 'NumTeamSeed', 'DenTeamSeed', 'NumTeamElo', 'DenTeamElo', 'HistWinPct']\n"
     ]
    }
   ],
   "source": [
    "all_train_columns = stats_columns + extra_cols\n",
    "# train_columns_to_drop = [\"FTM\", \"FGA\", \"TS%\", \"eFG%\", \"4FACTOR\", \"FG_PCT\", \"FT/FGA\", \"3PAr\", \"STL%\", \"Ast\",\n",
    "#                          \"ASSIST_RATIO\", \"DR\", \"FGA3\", \"DEF_REB_PCT\"]\n",
    "\n",
    "# train_columns_to_drop = [\"FGM3\", \"FTM\", \"FGA3\", \"FTA\", \"4FACTOR\", \"DEF_REB_PCT\", \"FT_PCT\", \"DR\", \"ASSIST_RATIO\",\n",
    "#                          \"Stl\", \"FG_PCT\", \"FGA\", \"PF\", \"FT_RATE\", \"OR\", \"Ast\", \"TO\", \"Blk\"]\n",
    "\n",
    "train_columns_to_drop = []\n",
    "\n",
    "train_columns = [c for c in all_train_columns if c not in train_columns_to_drop]\n",
    "# train_columns = ['FGM', 'DEF_EFF', 'WINPCT', 'OFF_EFF', 'DenTeamSeed', 'PIE', 'DenTeamElo', 'NumTeamSeed', 'NumTeamElo', 'HistWinPct']\n",
    "\n",
    "# train_columns = [\"NumTeamElo\", \"DenTeamElo\", \"ORB%\", \"TOV%\", \"Pace\", \"ORtg\"]\n",
    "print(train_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(914, 30)\n",
      "(914,)\n",
      "(67, 30)\n",
      "(67,)\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "seasons_test = [2017]\n",
    "seasons_train = [season for season in seasons if season not in seasons_test]\n",
    "\n",
    "train_x = pred_data.loc[pred_data[\"Season\"].isin(seasons_train)][train_columns].as_matrix()\n",
    "train_y = pred_data.loc[pred_data[\"Season\"].isin(seasons_train)][\"NumTeamWon\"].as_matrix()\n",
    "test_x = pred_data.loc[pred_data[\"Season\"].isin(seasons_test)][train_columns].as_matrix()\n",
    "test_y = pred_data.loc[pred_data[\"Season\"].isin(seasons_test)][\"NumTeamWon\"].as_matrix()\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "print(len(train_columns))\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# # train_y = to_categorical(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10], 'tol': [0.0001, 0.001, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10],\n",
    "              'tol': [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "              }\n",
    "\n",
    "logReg = GridSearchCV(LogisticRegression(), parameters, cv=10)\n",
    "\n",
    "logReg.fit(train_x, train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'tol': 0.0001}\n",
      "Accuracy, Test:   0.7761194029850746\n",
      "Log Loss, Test:   0.5122243365950685\n"
     ]
    }
   ],
   "source": [
    "preds_train = np.array(logReg.predict_proba(train_x))\n",
    "preds_test_lr = np.array(logReg.predict_proba(test_x))[:,1]\n",
    "\n",
    "# print(preds_test)\n",
    "# print(test_y)\n",
    "\n",
    "# threshold = .1\n",
    "\n",
    "# preds_test_lr[preds_test_lr > 1 - threshold] = 1\n",
    "# preds_test_lr[preds_test_lr < threshold] = 0\n",
    "\n",
    "print(logReg.best_params_)\n",
    "\n",
    "# print(\"Accuracy, Train: \", accuracy_score(train_y, preds_train > 0.5))\n",
    "# print(\"Log Loss, Train: \", log_loss(train_y, preds_train))\n",
    "# print(\"\")\n",
    "print(\"Accuracy, Test:  \", logReg.score(test_x, test_y))\n",
    "print(\"Log Loss, Test:  \", log_loss(test_y, preds_test_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=4, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=40, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "parameters = {'max_features': np.arange(1,12,1),\n",
    "              'min_samples_split': np.arange(2,50,4),\n",
    "              'min_samples_leaf': np.arange(1,25,4)\n",
    "              }\n",
    "\n",
    "# RF = GridSearchCV(RandomForestRegressor(n_estimators=1000, n_jobs=-1, random_state=0), parameters,\n",
    "#                     cv=10, scoring=make_scorer(log_loss))\n",
    "RF = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=0, max_features=4, min_samples_leaf=40)\n",
    "\n",
    "# clf = ExtraTreesRegressor(n_estimators=1000, max_features=1, n_jobs=-1)\n",
    "# clf = LogisticRegression(n_jobs=-1)\n",
    "RF.fit(train_x, train_y.ravel())\n",
    "# print(\"OOB Score: \", clf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Test:   0.746268656716418\n",
      "Log Loss, Test:   0.5385038508794131\n"
     ]
    }
   ],
   "source": [
    "# print(RF.best_params_)\n",
    "\n",
    "preds_train = RF.predict(train_x)\n",
    "preds_test_rf = RF.predict_proba(test_x)[:,1]\n",
    "# preds_test_rf = RF.predict_proba(test_x)\n",
    "\n",
    "# threshold = .1\n",
    "# preds_test_rf[preds_test_rf > 1 - threshold] = 1\n",
    "# preds_test_rf[preds_test_rf < threshold] = 0\n",
    "\n",
    "# print(\"Accuracy, Train: \", accuracy_score(train_y, preds_train > 0.5))\n",
    "# print(\"Log Loss, Train: \", log_loss(train_y, preds_train))\n",
    "# print(\"\")\n",
    "print(\"Accuracy, Test:  \", accuracy_score(test_y, preds_test_rf > 0.5))\n",
    "print(\"Log Loss, Test:  \", log_loss(test_y, preds_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGM3 : 0.0038384700109756077\n",
      "FTM : 0.0038607200920063116\n",
      "FGA3 : 0.0043999904518818665\n",
      "FTA : 0.004485741497124605\n",
      "4FACTOR : 0.0046001269954303965\n",
      "DEF_REB_PCT : 0.0051692381893911656\n",
      "FT_PCT : 0.005772051804293564\n",
      "DR : 0.005875924242828527\n",
      "ASSIST_RATIO : 0.006343233338749744\n",
      "Stl : 0.006512221982256073\n",
      "FG_PCT : 0.007898503219778438\n",
      "FGA : 0.008757772421568452\n",
      "PF : 0.008763457809594009\n",
      "FT_RATE : 0.009760742682694994\n",
      "OR : 0.01090169886295754\n",
      "Ast : 0.010947606365233944\n",
      "TO : 0.01126904518864288\n",
      "Blk : 0.015306405806461733\n",
      "OFF_REB_PCT : 0.017880172272048817\n",
      "TURNOVER_RATE : 0.020009118922276085\n",
      "FGM : 0.02592105182410565\n",
      "DEF_EFF : 0.0306102841619177\n",
      "WINPCT : 0.049093207247669746\n",
      "OFF_EFF : 0.05200256399167118\n",
      "DenTeamSeed : 0.0719617581985144\n",
      "PIE : 0.07669910607643693\n",
      "DenTeamElo : 0.07791245467141034\n",
      "NumTeamSeed : 0.09444445423518727\n",
      "NumTeamElo : 0.1292776528470162\n",
      "HistWinPct : 0.2197252245898757\n"
     ]
    }
   ],
   "source": [
    "variables = train_columns\n",
    "feature_importance = RF.feature_importances_\n",
    "\n",
    "feature_importance, variables = (list(t) for t in zip(*sorted(zip(feature_importance, variables))))\n",
    "\n",
    "# features_to_keep = []\n",
    "for i in range(len(feature_importance)):\n",
    "    print(variables[i], \":\", feature_importance[i])\n",
    "#     if feature_importance[i] > 0.025:\n",
    "#         features_to_keep.append(variables[i])\n",
    "# print(features_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6567164179104478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "mlp.fit(train_x, train_y)\n",
    "print(mlp.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "# from sklearn.preprocessing import scale\n",
    "# knn = KNeighborsClassifier()\n",
    "\n",
    "# parameters = {'leaf_size': np.arange(1,51,2),\n",
    "#               'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "#               'n_neighbors': np.arange(1,11,1)\n",
    "#              }\n",
    "\n",
    "# # knn = KNeighborsClassifier()\n",
    "# knn = GridSearchCV(KNeighborsClassifier(), parameters, cv=10)\n",
    "# knn.fit(scale(train_x), train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(knn.best_params_)\n",
    "\n",
    "# preds_train = knn.predict(train_x)\n",
    "# # preds_test_rf = RF.predict_proba(test_x)[:,1]\n",
    "# preds_test_rf = knn.predict_proba(scale(test_x))[:,1]\n",
    "\n",
    "# # threshold = .1\n",
    "# # preds_test_rf[preds_test_rf > 1 - threshold] = 1\n",
    "# # preds_test_rf[preds_test_rf < threshold] = 0\n",
    "\n",
    "# # print(\"Accuracy, Train: \", accuracy_score(train_y, preds_train > 0.5))\n",
    "# # print(\"Log Loss, Train: \", log_loss(train_y, preds_train))\n",
    "# # print(\"\")\n",
    "# print(\"Accuracy, Test:  \", knn.score(scale(test_x), test_y))\n",
    "# print(\"Log Loss, Test:  \", log_loss(test_y, preds_test_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71717172 0.65656566 0.65656566 0.71717172 0.70707071 0.67346939\n",
      " 0.6185567  0.65979381 0.64948454 0.77319588]\n",
      "Accuracy: 0.68 (+/- 0.09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "logReg = LogisticRegression(C=1, tol=0.001)\n",
    "RF = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=0, max_features=4, min_samples_leaf=40)\n",
    "mlp = MLPClassifier()\n",
    "knn = KNeighborsClassifier(n_neighbors=7, algorithm='auto', leaf_size=1)\n",
    "voter = VotingClassifier(estimators=[('lr', logReg), ('rf', RF), ('mlp', mlp), ('knn', knn)], voting='soft')\n",
    "scores= cross_val_score(voter, np.concatenate((test_x, train_x)),np.concatenate((test_y, train_y)), cv=10)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "# voter.fit(train_x, train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Test:   0.746268656716418\n",
      "Log Loss, Test:   0.4958949977095333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "voter.fit(train_x, train_y.ravel())\n",
    "preds_test = voter.predict_proba(test_x)[:,1]\n",
    "# threshold = .19\n",
    "# preds_test[preds_test > 1 - threshold] = 1\n",
    "# preds_test[preds_test < threshold] = 0\n",
    "print(\"Accuracy, Test:  \", voter.score(test_x, test_y))\n",
    "print(\"Log Loss, Test:  \", log_loss(test_y, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# knn = KNeighborsClassifier(n_neighbors=7, algorithm='auto', leaf_size=1)\n",
    "# logReg = LogisticRegression(C=10, tol=0.0001)\n",
    "# RF = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=0, max_features=4, min_samples_leaf=40)\n",
    "\n",
    "# voter = VotingClassifier(estimators=[('lr', logReg), ('rf', RF), ('knn', knn)], voting='soft')\n",
    "# voter.fit(train_x, train_y)\n",
    "# print(\"Accuracy: \", voter.score(test_x, test_y))\n",
    "\n",
    "\n",
    "# scores2 = cross_val_score(voter, np.concatenate((test_x, train_x)),np.concatenate((test_y, train_y)), cv=4)\n",
    "# print(scores2)\n",
    "# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores2.mean(), scores2.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
