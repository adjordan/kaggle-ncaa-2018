{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.metrics import accuracy_score, log_loss, make_scorer\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "root = \"/home/austin/Github/kaggle-ncaa-2018/\"\n",
    "\n",
    "data = pd.read_csv(root + \"derived_data/Master.csv\")\n",
    "matchups = pd.read_csv(\"/home/austin/Github/DataFiles/TourneyWinratesBySeed.csv\")\n",
    "tour_ratios = pd.read_csv(root + \"derived_data/ratios/NCAATourneyDetailedResultsRatios.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = range(2003, 2018)\n",
    "seasons_test = [2017]\n",
    "seasons_train = [season for season in seasons if season not in seasons_test]\n",
    "\n",
    "all_columns = data.columns.tolist()\n",
    "non_stats_columns = [\"Season\", \"TeamID\", \"Seed\", \"Elo\"]#, \"BPI\", \"Predictor_Score\"]\n",
    "stats_columns = [c for c in all_columns if c not in non_stats_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_games = tour_ratios.loc[tour_ratios[\"Season\"].isin(seasons)]\n",
    "tourney_games = tourney_games.reset_index(drop=True)\n",
    "extra_cols = [\"NumTeamSeed\", \"DenTeamSeed\", \"NumTeamElo\", \"DenTeamElo\", \"HistWinPct\"]\n",
    "\n",
    "pred_data = pd.DataFrame(index=range(tourney_games.shape[0]), columns= [\"Season\"] + stats_columns + extra_cols + [\"NumTeamWon\"])\n",
    "pred_data.loc[:,\"NumTeamWon\"] = np.ones(tourney_games.shape[0])\n",
    "index = 0\n",
    "for i in range(len(seasons)):\n",
    "    season = seasons[i]\n",
    "    tourney_games_for_season = tourney_games.loc[tourney_games[\"Season\"] == season].reset_index()\n",
    "    for j, row in tourney_games_for_season.iterrows():\n",
    "        pred_data.loc[index, \"Season\"] = season\n",
    "        teamA_id = tourney_games_for_season.at[j, \"WTeamID\"]\n",
    "        teamB_id = tourney_games_for_season.at[j, \"LTeamID\"]\n",
    "        \n",
    "        teamA_seed = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Seed\"]\n",
    "        teamB_seed = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Seed\"]\n",
    "        \n",
    "        teamA_elo = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Elo\"]\n",
    "        teamB_elo = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Elo\"]\n",
    "        \n",
    "        teamA_stats = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season), stats_columns].as_matrix()\n",
    "        teamB_stats = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season), stats_columns].as_matrix()\n",
    "        \n",
    "#         teamA_bpi = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"BPI\"]\n",
    "#         teamB_bpi = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"BPI\"]\n",
    "        \n",
    "#         teamA_pred = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Predictor_Score\"]\n",
    "#         teamB_pred = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Predictor_Score\"]\n",
    "        \n",
    "        r = random.random()\n",
    "        \n",
    "        if r > 0.5:\n",
    "            pred_data.at[index, stats_columns] = (teamA_stats / teamB_stats).ravel()\n",
    "            pred_data.at[index, \"NumTeamSeed\"] = teamA_seed\n",
    "            pred_data.at[index, \"DenTeamSeed\"] = teamB_seed\n",
    "            pred_data.at[index, \"NumTeamElo\"] = teamA_elo\n",
    "            pred_data.at[index, \"DenTeamElo\"] = teamB_elo\n",
    "            pred_data.at[index, \"HistWinPct\"] = matchups[(matchups[\"WinSeed\"] == teamA_seed) & (matchups[\"LoseSeed\"] == teamB_seed)][\"1985\"]\n",
    "#             pred_data.at[index, \"NumTeamBPI\"] = teamA_bpi\n",
    "#             pred_data.at[index, \"DenTeamBPI\"] = teamB_bpi\n",
    "#             pred_data.at[index, \"NumTeamPred\"] = teamA_pred\n",
    "#             pred_data.at[index, \"DenTeamPred\"] = teamB_pred\n",
    "        else:\n",
    "            pred_data.at[index, stats_columns] = (teamB_stats / teamA_stats).ravel()\n",
    "            pred_data.at[index, \"NumTeamSeed\"] = teamB_seed\n",
    "            pred_data.at[index, \"DenTeamSeed\"] = teamA_seed\n",
    "            pred_data.at[index, \"NumTeamElo\"] = teamB_elo\n",
    "            pred_data.at[index, \"DenTeamElo\"] = teamA_elo\n",
    "            pred_data.at[index, \"HistWinPct\"] = matchups[(matchups[\"WinSeed\"] == teamB_seed) & (matchups[\"LoseSeed\"] == teamA_seed)][\"1985\"]\n",
    "#             pred_data.at[index, \"NumTeamBPI\"] = teamB_bpi\n",
    "#             pred_data.at[index, \"DenTeamBPI\"] = teamA_bpi\n",
    "#             pred_data.at[index, \"NumTeamPred\"] = teamB_pred\n",
    "#             pred_data.at[index, \"DenTeamPred\"] = teamA_pred\n",
    "            pred_data.at[index, \"NumTeamWon\"] = 0\n",
    "        \n",
    "        index += 1\n",
    "\n",
    "pred_data = pred_data.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_columns = stats_columns + extra_cols\n",
    "train_columns_to_drop = [\"FT_PCT\", \"FGM3\", \"FGA3\", \"FTM\", \"DEF_REB_PCT\", \"FTA\", \"FT_RATE\", \"4Factor\",\n",
    "                         \"DR\", \"FG_PCT\", \"Stl\", \"ASSIST_RATIO\", \"PF\", \"TO\", \"Ast\", \"FGA\", \"OR\"]\n",
    "\n",
    "# train_columns_to_drop = []\n",
    "\n",
    "train_columns = [c for c in all_train_columns if c not in train_columns_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pred_data.loc[pred_data[\"Season\"].isin(seasons_train)][train_columns].as_matrix()\n",
    "train_y = pred_data.loc[pred_data[\"Season\"].isin(seasons_train)][\"NumTeamWon\"].as_matrix()\n",
    "test_x = pred_data.loc[pred_data[\"Season\"].isin(seasons_test)][train_columns].as_matrix()\n",
    "test_y = pred_data.loc[pred_data[\"Season\"].isin(seasons_test)][\"NumTeamWon\"].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# parameters = {'max_features': np.arange(1,12,1),\n",
    "#               'min_samples_split': np.arange(2,50,2),\n",
    "#               'min_samples_leaf': np.arange(1,15,1)\n",
    "#               }\n",
    "\n",
    "# clf = RandomizedSearchCV(ExtraTreesRegressor(n_estimators=1000, n_jobs=-1, random_state=0), parameters,\n",
    "#                    cv=10, scoring=make_scorer(log_loss), n_iter=50)\n",
    "\n",
    "# clf = ExtraTreesRegressor(n_estimators=1000, n_jobs=-1, random_state=0, max_features=4, min_samples_split=40)\n",
    "\n",
    "# clf.fit(train_x, train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preds_train = clf.predict(train_x)\n",
    "# preds_test = clf.predict(test_x)\n",
    "\n",
    "# print(clf.best_params_)\n",
    "\n",
    "# # print(\"Accuracy, Train: \", accuracy_score(train_y, preds_train > 0.5))\n",
    "# # print(\"Log Loss, Train: \", log_loss(train_y, preds_train))\n",
    "# # print(\"\")\n",
    "# print(\"Accuracy, Test:  \", accuracy_score(test_y, preds_test > 0.5))\n",
    "# print(\"Log Loss, Test:  \", log_loss(test_y, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=20, n_jobs=1,\n",
       "          param_distributions={'min_samples_split': array([ 2,  6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46]), 'max_features': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), 'min_samples_leaf': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=make_scorer(log_loss),\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "parameters = {'max_features': np.arange(1,12,1),\n",
    "              'min_samples_split': np.arange(2,50,4),\n",
    "              'min_samples_leaf': np.arange(1,15,1)\n",
    "              }\n",
    "\n",
    "clf1 = RandomizedSearchCV(RandomForestRegressor(n_estimators=1000, n_jobs=-1, random_state=0), parameters,\n",
    "                    cv=10, scoring=make_scorer(log_loss), n_iter=20)\n",
    "# clf1 = RandomForestRegressor(n_estimators=1000, n_jobs=-1, random_state=0, max_features=5, min_samples_split=40)\n",
    "\n",
    "# clf = ExtraTreesRegressor(n_estimators=1000, max_features=1, n_jobs=-1)\n",
    "# clf = LogisticRegression(n_jobs=-1)\n",
    "clf1.fit(train_x, train_y.ravel())\n",
    "# print(\"OOB Score: \", clf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 10, 'max_features': 10, 'min_samples_leaf': 1}\n",
      "Accuracy, Train:  0.9781181619256017\n",
      "Log Loss, Train:  0.241729193049447\n",
      "Accuracy, Test:   0.746268656716418\n",
      "Log Loss, Test:   0.5581382700221917\n"
     ]
    }
   ],
   "source": [
    "preds_train1 = clf1.predict(train_x)\n",
    "preds_test1 = clf1.predict(test_x)\n",
    "\n",
    "# threshold = .1\n",
    "\n",
    "# preds_test1[preds_test1 > 1 - threshold] = 1\n",
    "# preds_test1[preds_test1 < threshold] = 0\n",
    "\n",
    "print(clf1.best_params_)\n",
    "\n",
    "print(\"Accuracy, Train: \", accuracy_score(train_y, preds_train1 > 0.5))\n",
    "print(\"Log Loss, Train: \", log_loss(train_y, preds_train1))\n",
    "# print(\"\")\n",
    "print(\"Accuracy, Test:  \", accuracy_score(test_y, preds_test1 > 0.5))\n",
    "print(\"Log Loss, Test:  \", log_loss(test_y, preds_test1))\n",
    "\n",
    "variables = train_columns\n",
    "feature_importance = clf1.feature_importances_\n",
    "\n",
    "feature_importance, variables = (list(t) for t in zip(*sorted(zip(feature_importance, variables))))\n",
    "\n",
    "for i in range(len(feature_importance)):\n",
    "    print(variables[i], \":\", feature_importance[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4FACTOR : 0.02830725266323982\n",
      "Blk : 0.035794866139219214\n",
      "DEF_EFF : 0.03627180253409459\n",
      "OFF_REB_PCT : 0.03844534293907675\n",
      "FGM : 0.039907943916444394\n",
      "TURNOVER_RATE : 0.04065380783449319\n",
      "WINPCT : 0.04288766435369602\n",
      "DenTeamSeed : 0.04576789430324574\n",
      "OFF_EFF : 0.047571705193758106\n",
      "NumTeamSeed : 0.06932996407865333\n",
      "PIE : 0.07303979017973293\n",
      "DenTeamElo : 0.07644663015593192\n",
      "NumTeamElo : 0.13482640634995546\n",
      "HistWinPct : 0.2907489293584585\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
