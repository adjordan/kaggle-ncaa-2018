{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "root = \"/home/austin/Github/kaggle-ncaa-2018/\"\n",
    "\n",
    "data = pd.read_csv(root + \"derived_data/Master3.csv\")\n",
    "matchups = pd.read_csv(\"/home/austin/Github/DataFiles/TourneyWinratesBySeed.csv\")\n",
    "tour_ratios = pd.read_csv(root + \"derived_data/ratios/NCAATourneyDetailedResultsRatios.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = range(2010, 2018)\n",
    "\n",
    "all_columns = data.columns.tolist()\n",
    "non_stats_columns = [\"Season\", \"TeamID\", \"Seed\", \"Elo\", \"NumTeamID\", \"DenTeamID\"]#, \"BPI\", \"Predictor_Score\"]\n",
    "stats_columns = [c for c in all_columns if c not in non_stats_columns]\n",
    "# stats_columns_A = [\"A\"+c for c in all_columns if c not in non_stats_columns]\n",
    "# stats_columns_B = [\"B\"+c for c in all_columns if c not in non_stats_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all data in a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_games = tour_ratios.loc[tour_ratios[\"Season\"].isin(seasons)]\n",
    "tourney_games = tourney_games.reset_index(drop=True)\n",
    "extra_cols = [\"NumTeamSeed\", \"DenTeamSeed\", \"NumTeamElo\", \"DenTeamElo\", \"HistWinPct\", \"Upset\"]\n",
    "\n",
    "pred_data = pd.DataFrame(index=range(tourney_games.shape[0]), columns= [\"Season\", \"NumTeamID\", \"DenTeamID\"] + stats_columns + extra_cols + [\"NumTeamWon\"])\n",
    "pred_data.loc[:,\"NumTeamWon\"] = np.ones(tourney_games.shape[0])\n",
    "pred_data.loc[:,\"Upset\"] = np.zeros(tourney_games.shape[0])\n",
    "                                    \n",
    "index = 0\n",
    "for i in range(len(seasons)):\n",
    "    season = seasons[i]\n",
    "    tourney_games_for_season = tourney_games.loc[tourney_games[\"Season\"] == season].reset_index()\n",
    "    for j, row in tourney_games_for_season.iterrows():\n",
    "        pred_data.loc[index, \"Season\"] = season\n",
    "        teamA_id = tourney_games_for_season.at[j, \"WTeamID\"]\n",
    "        teamB_id = tourney_games_for_season.at[j, \"LTeamID\"]\n",
    "        \n",
    "        teamA_seed = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Seed\"]\n",
    "        teamB_seed = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Seed\"]\n",
    "        \n",
    "        teamA_elo = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Elo\"]\n",
    "        teamB_elo = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Elo\"]\n",
    "        \n",
    "        teamA_stats = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season), stats_columns].as_matrix()\n",
    "        teamB_stats = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season), stats_columns].as_matrix()\n",
    "        \n",
    "        r = random.random()\n",
    "        \n",
    "        if r > 0.5:\n",
    "            pred_data.at[index, stats_columns] = (teamA_stats / teamB_stats).ravel()\n",
    "            pred_data.at[index, \"NumTeamID\"] = teamA_id\n",
    "            pred_data.at[index, \"DenTeamID\"] = teamB_id\n",
    "            pred_data.at[index, \"NumTeamSeed\"] = teamA_seed\n",
    "            pred_data.at[index, \"DenTeamSeed\"] = teamB_seed\n",
    "            pred_data.at[index, \"NumTeamElo\"] = teamA_elo\n",
    "            pred_data.at[index, \"DenTeamElo\"] = teamB_elo\n",
    "            pred_data.at[index, \"HistWinPct\"] = matchups[(matchups[\"WinSeed\"] == teamA_seed) & (matchups[\"LoseSeed\"] == teamB_seed)][\"1985\"]\n",
    "            \n",
    "        else:\n",
    "            pred_data.at[index, stats_columns] = (teamB_stats / teamA_stats).ravel()\n",
    "            pred_data.at[index, \"NumTeamID\"] = teamB_id\n",
    "            pred_data.at[index, \"DenTeamID\"] = teamA_id\n",
    "            pred_data.at[index, \"NumTeamSeed\"] = teamB_seed\n",
    "            pred_data.at[index, \"DenTeamSeed\"] = teamA_seed\n",
    "            pred_data.at[index, \"NumTeamElo\"] = teamB_elo\n",
    "            pred_data.at[index, \"DenTeamElo\"] = teamA_elo\n",
    "            pred_data.at[index, \"HistWinPct\"] = matchups[(matchups[\"WinSeed\"] == teamB_seed) & (matchups[\"LoseSeed\"] == teamA_seed)][\"1985\"]\n",
    "            pred_data.at[index, \"NumTeamWon\"] = 0\n",
    "            \n",
    "        if teamA_seed >= teamB_seed + 3:\n",
    "                pred_data.at[index, \"Upset\"] = 1\n",
    "        \n",
    "        index += 1\n",
    "\n",
    "pred_data = pred_data.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select which stats to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FGM', 'FGA', 'FGM3', 'FGA3', 'FTM', 'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', 'PIE', 'FG_PCT', 'TURNOVER_RATE', 'OFF_REB_PCT', 'FT_RATE', '4FACTOR', 'OFF_EFF', 'DEF_EFF', 'ASSIST_RATIO', 'DEF_REB_PCT', 'FT_PCT', 'WINPCT', 'Pace', 'ORtg', 'FTr', '3PAr', 'TS%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'eFG%', 'TOV%', 'ORB%', 'FT/FGA', 'NumTeamSeed', 'DenTeamSeed', 'NumTeamElo', 'DenTeamElo', 'HistWinPct', 'Upset', 'NumTeamWon']\n"
     ]
    }
   ],
   "source": [
    "all_train_columns = stats_columns + extra_cols + [\"NumTeamWon\"]\n",
    "\n",
    "# train_columns_to_drop = [\"FGA\", \"Ast\", \"FGM\", \"Blk\", \"STL%\", \"Stl\", \"DR\", \"NumTeamSeed\", \"NumTeamElo\", \"OR\", \"Pace\",\n",
    "#                          \"ASSIST_RATIO\", \"AST%\", \"PF\", \"DEF_EFF\", \"PIE\", \"FT_PCT\"]\n",
    "\n",
    "train_columns_to_drop = []\n",
    "\n",
    "train_columns = [c for c in all_train_columns if c not in train_columns_to_drop]\n",
    "print(train_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_columns = features_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2010, 2011, 2012, 2013, 2014, 2015, 2016]\n",
      "(196, 45)\n",
      "(196,)\n",
      "(67, 45)\n",
      "(67,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "seasons_test = [2017]\n",
    "seasons_train = [season for season in seasons if season not in seasons_test]\n",
    "print(seasons_train)\n",
    "\n",
    "# train_x = pred_data.loc[pred_data[\"Season\"].isin(seasons_train)][train_columns].as_matrix()\n",
    "# train_y = pred_data.loc[pred_data[\"Season\"].isin(seasons_train)][\"U\"].as_matrix()\n",
    "# test_x = pred_data.loc[pred_data[\"Season\"].isin(seasons_test)][train_columns].as_matrix()\n",
    "# test_y = pred_data.loc[pred_data[\"Season\"].isin(seasons_test)][\"NumTeamWon\"].as_matrix()\n",
    "\n",
    "pred_data_no_upset = pred_data[pred_data[\"Upset\"] == 0]\n",
    "pred_data_upset = pred_data[pred_data[\"Upset\"] == 1]\n",
    "num_upset = pred_data_upset.shape[0]\n",
    "pred_data_no_upset_sample = pred_data_no_upset.sample(num_upset)\n",
    "\n",
    "data_to_use = pd.concat([pred_data_upset, pred_data_no_upset_sample])\n",
    "data_to_use = shuffle(data_to_use)\n",
    "\n",
    "train_x = data_to_use.loc[data_to_use[\"Season\"].isin(seasons_train)][train_columns].as_matrix()\n",
    "train_y = data_to_use.loc[data_to_use[\"Season\"].isin(seasons_train)][\"Upset\"].as_matrix()\n",
    "test_x = pred_data.loc[pred_data[\"Season\"].isin(seasons_test)][train_columns].as_matrix()\n",
    "test_y = pred_data.loc[(pred_data[\"Season\"].isin(seasons_test)) & (pred_data[\"TeamA\"])][\"Upset\"].as_matrix()\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "\n",
    "# print(pred_data_no_upset)\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# # train_y = to_categorical(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10], 'tol': [0.0001, 0.001, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10],\n",
    "              'tol': [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "              }\n",
    "\n",
    "logReg = GridSearchCV(LogisticRegression(), parameters, cv=10)\n",
    "\n",
    "logReg.fit(train_x, train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "Log Loss, Test:   0.2886680312481349\n",
      "\n",
      "[0.17480871 0.08909212 0.12336426 0.19189308 0.18644835 0.24473435\n",
      " 0.23420471 0.21146608 0.26835234 0.24843216 0.70381561 0.21859215\n",
      " 0.19920083 0.21594288 0.2521763  0.18470174 0.23072965 0.19281275\n",
      " 0.22358907 0.73638217 0.2136455  0.2374623  0.22135954 0.19403274\n",
      " 0.25059165 0.17839448 0.19426041 0.26395794 0.23893487 0.18535091\n",
      " 0.25559525 0.7248086  0.20681181 0.22043245 0.76568569 0.74812356\n",
      " 0.25875917 0.22775919 0.30243576 0.3200402  0.32578664 0.32447721\n",
      " 0.81261597 0.77708909 0.27676314 0.26948224 0.28601845 0.74896315\n",
      " 0.3141394  0.24351432 0.81988891 0.30543724 0.33315205 0.32954514\n",
      " 0.25891054 0.74141973 0.26643117 0.36896722 0.28624594 0.82220086\n",
      " 0.26639245 0.31456652 0.3600041  0.79708788 0.31742165 0.37254661\n",
      " 0.3693121 ]\n",
      "     Season NumTeamID DenTeamID       FGM       FGA      FGM3      FGA3  \\\n",
      "466    2017      1448      1243   1.13349   1.10069   1.14143   1.06462   \n",
      "467    2017      1309      1291   1.01994  0.980246  0.582462  0.666977   \n",
      "468    2017      1413      1300  0.921935   0.96904  0.839041  0.796448   \n",
      "469    2017      1425      1344   1.11658   1.09925   1.09196   1.10623   \n",
      "470    2017      1112      1315  0.934732  0.935398  0.987373  0.954432   \n",
      "471    2017      1457      1139  0.992298   1.04343   1.26878   1.21401   \n",
      "472    2017      1190      1196   1.03525  0.941891  0.967742  0.916788   \n",
      "473    2017      1199      1195   1.05399   1.06104   1.09768   1.04529   \n",
      "474    2017      1355      1211  0.831587  0.947045   1.26232   1.33523   \n",
      "475    2017      1235      1305     1.103    1.0584   1.08644    1.0392   \n",
      "476    2017      1278      1292  0.936009   1.04968  0.957547   1.02967   \n",
      "477    2017      1321      1435   1.08312    1.0717  0.715134  0.789944   \n",
      "478    2017      1323      1343    1.0899   1.05763   0.95137  0.940206   \n",
      "479    2017      1345      1436   1.03705   1.07388   1.43478   1.29638   \n",
      "480    2017      1388      1433  0.976665  0.908879   1.52094   1.27356   \n",
      "481    2017      1437      1291   1.09868  0.980779   1.21951   1.17391   \n",
      "482    2017      1423      1438   1.20828   1.17647   1.35714   1.48246   \n",
      "483    2017      1452      1137   1.07642   1.13137  0.927798  0.968665   \n",
      "484    2017      1458      1439  0.970588   1.05215  0.854708  0.967952   \n",
      "485    2017      1462      1268   1.02661   1.02018  0.844619  0.911765   \n",
      "486    2017      1371      1116  0.953208  0.975776  0.938063   1.01025   \n",
      "487    2017      1308      1124   1.01523   1.02428   1.11902   1.18299   \n",
      "488    2017      1243      1153  0.918949  0.910714   0.95671   0.91099   \n",
      "489    2017      1181      1407    1.0871   1.01702   1.01001  0.951553   \n",
      "490    2017      1413      1242  0.813953  0.917138  0.702509  0.811321   \n",
      "491    2017      1246      1297   1.16765   1.10739  0.806723  0.824592   \n",
      "492    2017      1257      1240   1.14194   1.14544  0.991304   1.01435   \n",
      "493    2017      1276      1329  0.912494  0.878795   1.04127   1.09915   \n",
      "494    2017      1277      1274   1.04071   1.00797   1.22513   1.17009   \n",
      "495    2017      1314      1411   1.24271   1.14507   1.24365   1.00563   \n",
      "..      ...       ...       ...       ...       ...       ...       ...   \n",
      "503    2017      1292      1139   1.04226   1.02146  0.840302  0.825572   \n",
      "504    2017      1438      1196  0.936545  0.907026  0.903226  0.832117   \n",
      "505    2017      1321      1211   0.83304  0.989649  0.970588   1.08749   \n",
      "506    2017      1235      1345   1.05322    1.0779   1.10774   1.11878   \n",
      "507    2017      1323      1452  0.950304  0.944264   1.24514   1.16737   \n",
      "508    2017      1458      1437  0.975904   1.07349  0.856667  0.892593   \n",
      "509    2017      1462      1199   0.86582   0.92139    1.0219   1.07115   \n",
      "510    2017      1425      1124   1.05461    1.0848   1.18684   1.15481   \n",
      "511    2017      1242      1277   1.14772   1.10276   1.11088   1.03125   \n",
      "512    2017      1455      1246  0.920846  0.931199   1.23636   1.07279   \n",
      "513    2017      1257      1276   1.06165    1.1259   0.76029  0.808866   \n",
      "514    2017      1116      1314  0.899081  0.913022  0.901551  0.908066   \n",
      "515    2017      1348      1332  0.931744  0.992171  0.728873     0.812   \n",
      "516    2017      1376      1181  0.877099   1.01835  0.807572  0.909416   \n",
      "517    2017      1417      1153   1.25189   1.09572   1.33694   1.13277   \n",
      "518    2017      1211      1452   1.04702  0.925981   0.96616  0.914376   \n",
      "519    2017      1242      1345   1.05817   1.04238   0.96875  0.971791   \n",
      "520    2017      1332      1276   1.06406   1.06673   0.91833  0.924947   \n",
      "521    2017      1112      1462   1.01712  0.970969  0.958159  0.819346   \n",
      "522    2017      1196      1458    1.0148   1.01994   1.02529   1.00666   \n",
      "523    2017      1417      1246   1.11767   1.02647   1.40379   1.22453   \n",
      "524    2017      1314      1139   1.15079   1.17424  0.947321  0.939394   \n",
      "525    2017      1376      1124  0.941176   1.06951   1.00488   1.06379   \n",
      "526    2017      1462      1211  0.848538  0.967579  0.962534   1.08134   \n",
      "527    2017      1242      1332   1.05695   1.04578   1.01309  0.947375   \n",
      "528    2017      1314      1246   1.02227   1.04131   1.02601  0.989334   \n",
      "529    2017      1376      1196  0.912172  0.998449   0.85744  0.929786   \n",
      "530    2017      1376      1211  0.798918    1.0073  0.909918    1.0409   \n",
      "531    2017      1332      1314   0.90668   0.88092   1.18828   1.15031   \n",
      "532    2017      1211      1314  0.984283  0.889655   1.00837  0.967791   \n",
      "\n",
      "          FTM       FTA        OR    ...          TOV%      ORB%    FT/FGA  \\\n",
      "466   1.21931   1.08823   1.12321    ...       0.85119   1.04068   1.10839   \n",
      "467   1.36644   1.29919   1.55989    ...       1.18129   1.59565   1.38991   \n",
      "468   1.09337   1.14566  0.926171    ...       1.18919  0.878419   1.19313   \n",
      "469   1.14677   1.07162   1.20283    ...      0.840491   1.17045   1.03802   \n",
      "470   1.08438   1.04714   1.14835    ...      0.923567   1.23507   1.19231   \n",
      "471   1.09099    1.1219   1.08272    ...       1.17164         1  0.967273   \n",
      "472  0.981982   1.02225   0.82973    ...       1.23288  0.984277   1.03754   \n",
      "473   1.17231   1.17828   1.04318    ...      0.921569   0.99115   1.14719   \n",
      "474   1.05014   1.00617   0.96784    ...       1.15493  0.919463   1.11071   \n",
      "475  0.691502  0.699091  0.843972    ...      0.940299  0.844884  0.666667   \n",
      "476   1.41221   1.39929   1.16875    ...       1.02941  0.964169   1.32547   \n",
      "477  0.915663  0.945483   1.31183    ...      0.846626   1.24291  0.837638   \n",
      "478   1.30061   1.19767   1.15004    ...      0.911111   1.11885   1.27933   \n",
      "479    1.1516   1.05604   1.07209    ...       1.07333    1.0132   1.05439   \n",
      "480  0.704167  0.666089  0.872667    ...      0.993464   1.05769  0.805447   \n",
      "481   1.24096   1.05528   1.11923    ...      0.877193   1.30435   1.26606   \n",
      "482    1.3738   1.41176   1.34043    ...      0.905109   1.16312   1.19022   \n",
      "483    1.1625   1.16407   1.66212    ...      0.839286   1.41199   1.05138   \n",
      "484  0.729636  0.831921   1.68929    ...      0.922581   1.52564  0.704319   \n",
      "485   1.06829   1.08863    1.1751    ...       0.96319   1.10611   1.01799   \n",
      "486  0.804339  0.952771   1.15357    ...       1.14685   1.12618  0.834483   \n",
      "487   1.31583   1.32076  0.983376    ...      0.964912  0.912935    1.2582   \n",
      "488   1.14759    1.1371  0.766624    ...       1.30233  0.830986   1.28251   \n",
      "489   1.02445  0.975132  0.982163    ...      0.928105         1   1.04912   \n",
      "490   1.01681   1.02378  0.849351    ...       1.16556  0.852507   1.13934   \n",
      "491   1.28342   1.23883   1.14765    ...      0.817073   1.04732   1.16599   \n",
      "492    1.1253   1.17789   1.31889    ...      0.744444    1.1619   0.94332   \n",
      "493  0.698024  0.708815  0.550692    ...      0.847682  0.643045  0.774834   \n",
      "494   0.91795  0.992323  0.857387    ...       1.08434  0.871257  0.910569   \n",
      "495  0.813987  0.845648   1.35635    ...      0.937063   1.30696  0.748466   \n",
      "..        ...       ...       ...    ...           ...       ...       ...   \n",
      "503  0.792236  0.836508   1.10517    ...       1.01493   1.13284  0.770909   \n",
      "504  0.563964  0.578534  0.762162    ...      0.938356  0.886792  0.627986   \n",
      "505  0.810601  0.796145   1.14963    ...      0.971831    1.0302  0.810714   \n",
      "506  0.790859  0.868023  0.948148    ...      0.782609  0.833876  0.769841   \n",
      "507  0.820789  0.696233  0.650924    ...       0.87234  0.724138  0.860902   \n",
      "508  0.790291  0.973806    1.4433    ...      0.953333      1.19  0.768116   \n",
      "509  0.999889   1.00607   1.03546    ...       1.11348   1.02381   1.06792   \n",
      "510   1.22549   1.17985  0.850952    ...       0.80117  0.768657   1.11885   \n",
      "511   1.20313   1.20089   1.26847    ...      0.838889   1.16495   1.08929   \n",
      "512  0.950168  0.907675  0.946049    ...       1.03731   1.03012   1.02083   \n",
      "513   1.13507   1.28389   1.77163    ...       1.04688   1.49388  0.995726   \n",
      "514   1.15592   1.07183  0.718609    ...       1.06716  0.767554   1.18852   \n",
      "515   1.06009   1.13939   1.15634    ...      0.952381   1.01863   1.06073   \n",
      "516   0.95374   1.04614    1.1969    ...       1.09155   1.06013  0.929766   \n",
      "517  0.986607  0.904656  0.786155    ...       1.03876  0.814085  0.887892   \n",
      "518   1.00815  0.926396  0.653724    ...       1.00709  0.790451   1.05263   \n",
      "519   0.97541   1.11894   1.22222    ...      0.937888   1.10423  0.968254   \n",
      "520   1.09832   1.20482   1.36709    ...       1.14844   1.31429   1.05556   \n",
      "521   1.06593  0.967254  0.843373    ...      0.923567  0.962209   1.09541   \n",
      "522   1.44886   1.28441  0.936012    ...       1.02098  0.890756   1.38208   \n",
      "523  0.701587  0.660127  0.775135    ...             1  0.870482    0.6875   \n",
      "524   1.02406    1.0721    1.7959    ...             1   1.52399  0.887273   \n",
      "525   1.21343   1.24915  0.987277    ...      0.906433  0.833333   1.13934   \n",
      "526  0.970588   1.04141   1.30354    ...       1.10563   1.15436   1.01071   \n",
      "527   1.05338   1.11719   1.17118    ...       1.02721    1.0528  0.987854   \n",
      "528  0.830784  0.824013   1.25177    ...             1   1.24398  0.847222   \n",
      "529  0.941122  0.989022   1.08248    ...       1.06164   1.05346  0.948805   \n",
      "530   0.98653   1.05301   1.33667    ...       1.09155   1.12416  0.992857   \n",
      "531  0.917323  0.917942  0.651923    ...       1.09701  0.779661    1.0123   \n",
      "532    1.0748   1.02921  0.594231    ...        1.0597   0.72155   1.14754   \n",
      "\n",
      "    NumTeamSeed DenTeamSeed NumTeamElo DenTeamElo  \\\n",
      "466          11          11    1730.44    1783.76   \n",
      "467          16          16     1395.7    1396.58   \n",
      "468          16          16    1496.14    1458.39   \n",
      "469          11          11     1766.4    1821.47   \n",
      "470           2          15    2041.92    1504.75   \n",
      "471          13           4    1543.08    1892.83   \n",
      "472          13           4    1603.29    1944.48   \n",
      "473           3          14    1866.09    1547.98   \n",
      "474          16           1     1607.5    2041.06   \n",
      "475           5          12    1944.94     1757.4   \n",
      "476           5          12    1751.52    1766.37   \n",
      "477           8           9    1770.25    1813.66   \n",
      "478           5          12    1916.33    1710.38   \n",
      "479           4          13    1940.64    1652.42   \n",
      "480           7          10    1879.27    1841.22   \n",
      "481           1          16     2097.4    1396.58   \n",
      "482          12           5       1696    2009.79   \n",
      "483           4          13     1973.5     1596.4   \n",
      "484           8           9    1936.38    1746.41   \n",
      "485          11           6    1856.66    1858.58   \n",
      "486           9           8    1839.45    1809.06   \n",
      "487          14           3    1678.28    1954.22   \n",
      "488          11           6    1783.76    1906.94   \n",
      "489           2          15     2021.2    1447.51   \n",
      "490          16           1    1496.14     2079.1   \n",
      "491           2          15    2070.61    1506.15   \n",
      "492           2          15    2009.24    1408.16   \n",
      "493           7          10    1904.71    1830.61   \n",
      "494           9           8    1892.04    1874.38   \n",
      "495           1          16    2053.06    1450.49   \n",
      "..          ...         ...        ...        ...   \n",
      "503          12           4    1766.37    1892.83   \n",
      "504           5           4    2009.79    1944.48   \n",
      "505           8           1    1770.25    2041.06   \n",
      "506           5           4    1944.94    1940.64   \n",
      "507           5           4    1916.33     1973.5   \n",
      "508           8           1    1936.38     2097.4   \n",
      "509          11           3    1856.66    1866.09   \n",
      "510          11           3     1766.4    1954.22   \n",
      "511           1           9     2079.1    1892.04   \n",
      "512          10           2    1956.88    2070.61   \n",
      "513           2           7    2009.24    1904.71   \n",
      "514           8           1    1809.06    2053.06   \n",
      "515          11           3    1790.62    2000.08   \n",
      "516           7           2    1801.42     2021.2   \n",
      "517           3           6    1904.86    1906.94   \n",
      "518           1           4    2041.06     1973.5   \n",
      "519           1           4     2079.1    1940.64   \n",
      "520           3           7    2000.08    1904.71   \n",
      "521           2          11    2041.92    1856.66   \n",
      "522           4           8    1944.48    1936.38   \n",
      "523           3           2    1904.86    2070.61   \n",
      "524           1           4    2053.06    1892.83   \n",
      "525           7           3    1801.42    1954.22   \n",
      "526          11           1    1856.66    2041.06   \n",
      "527           1           3     2079.1    2000.08   \n",
      "528           1           2    2053.06    2070.61   \n",
      "529           7           4    1801.42    1944.48   \n",
      "530           7           1    1801.42    2041.06   \n",
      "531           3           1    2000.08    2053.06   \n",
      "532           1           1    2041.06    2053.06   \n",
      "\n",
      "                                     HistWinPct Upset NumTeamWon  \n",
      "466       170    0.5\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "467       255    0.5\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "468       255    0.5\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "469       170    0.5\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "470   30    0.939394\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "471   195    0.19697\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "472   195    0.19697\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "473   45    0.840909\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "474       240    0.0\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "475   75    0.643939\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "476   75    0.643939\n",
      "Name: 1985, dtype: float64   1.0        0.0  \n",
      "477  120    0.507576\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "478   75    0.643939\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "479    60    0.80303\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "480  105    0.613636\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "481        15    1.0\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "482  180    0.356061\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "483    60    0.80303\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "484  120    0.507576\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "485  165    0.371212\n",
      "Name: 1985, dtype: float64   1.0        1.0  \n",
      "486  135    0.492424\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "487  210    0.159091\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "488  165    0.371212\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "489   30    0.939394\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "490       240    0.0\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "491   30    0.939394\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "492   30    0.939394\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "493  105    0.613636\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "494  135    0.492424\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "495        15    1.0\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "..                                          ...   ...        ...  \n",
      "503  179    0.333333\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "504    67    0.43662\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "505  112    0.202899\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "506    67    0.43662\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "507    67    0.43662\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "508  112    0.202899\n",
      "Name: 1985, dtype: float64   1.0        1.0  \n",
      "509  162    0.333333\n",
      "Name: 1985, dtype: float64   1.0        1.0  \n",
      "510  162    0.333333\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "511    8    0.924242\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "512  145    0.391304\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "513   22    0.705128\n",
      "Name: 1985, dtype: float64   1.0        0.0  \n",
      "514  112    0.202899\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "515  162    0.333333\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "516   97    0.294872\n",
      "Name: 1985, dtype: float64   1.0        1.0  \n",
      "517    37    0.58209\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "518    3    0.721311\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "519    3    0.721311\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "520   38    0.571429\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "521   26    0.866667\n",
      "Name: 1985, dtype: float64   1.0        0.0  \n",
      "522   55    0.444444\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "523       33    0.36\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "524    3    0.721311\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "525   98    0.428571\n",
      "Name: 1985, dtype: float64   1.0        1.0  \n",
      "526  160    0.428571\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "527    2    0.566667\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "528    1    0.539683\n",
      "Name: 1985, dtype: float64   0.0        1.0  \n",
      "529        99    0.6\n",
      "Name: 1985, dtype: float64   1.0        1.0  \n",
      "530   96    0.142857\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "531   32    0.433333\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "532         0    0.5\n",
      "Name: 1985, dtype: float64   0.0        0.0  \n",
      "\n",
      "[67 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "# preds_train = np.array(logReg.predict_proba(train_x))\n",
    "preds_test_lr = np.array(logReg.predict_proba(test_x))[:,1]\n",
    "\n",
    "# print(preds_test)\n",
    "# print(test_y)\n",
    "\n",
    "# threshold = .1\n",
    "\n",
    "# preds_test_lr[preds_test_lr > 1 - threshold] = 1\n",
    "# preds_test_lr[preds_test_lr < threshold] = 0\n",
    "\n",
    "# print(logReg.best_params_)\n",
    "\n",
    "# print(\"Accuracy, Train: \", accuracy_score(train_y, preds_train > 0.5))\n",
    "# print(\"Log Loss, Train: \", log_loss(train_y, preds_train))\n",
    "# print(\"\")\n",
    "print(\"Accuracy: \", logReg.score(test_x,test_y))\n",
    "# print(\"Accuracy, Test:  \", accuracy_score(test_y, preds_test_lr > 0.5))\n",
    "print(\"Log Loss, Test:  \", log_loss(test_y, preds_test_lr))\n",
    "print(\"\")\n",
    "print(preds_test_lr)\n",
    "print(pred_data.loc[pred_data[\"Season\"].isin(seasons_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=1, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=30, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "parameters = {'max_features': np.arange(1,12,1),\n",
    "              'min_samples_split': np.arange(2,50,4),\n",
    "              'min_samples_leaf': np.arange(1,25,4)\n",
    "              }\n",
    "\n",
    "# RF = RandomizedSearchCV(RandomForestRegressor(n_estimators=1000, n_jobs=-1, random_state=0), parameters,\n",
    "#                     cv=10, n_iter=10)\n",
    "RF = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=0, max_features=1, min_samples_leaf=30)\n",
    "\n",
    "# clf = ExtraTreesRegressor(n_estimators=1000, max_features=1, n_jobs=-1)\n",
    "# clf = LogisticRegression(n_jobs=-1)\n",
    "RF.fit(train_x, train_y.ravel())\n",
    "# print(\"OOB Score: \", clf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Test:   0.6865671641791045\n",
      "Log Loss, Test:   10.825813382750718\n"
     ]
    }
   ],
   "source": [
    "# print(RF.best_params_)\n",
    "\n",
    "preds_train = RF.predict(train_x)\n",
    "# preds_test_rf = RF.predict_proba(test_x)[:,1]\n",
    "preds_test_rf = RF.predict(test_x)\n",
    "\n",
    "# threshold = .1\n",
    "# preds_test_rf[preds_test_rf > 1 - threshold] = 1\n",
    "# preds_test_rf[preds_test_rf < threshold] = 0\n",
    "\n",
    "# print(\"Accuracy, Train: \", accuracy_score(train_y, preds_train > 0.5))\n",
    "# print(\"Log Loss, Train: \", log_loss(train_y, preds_train))\n",
    "# print(\"\")\n",
    "print(\"Accuracy, Test:  \", accuracy_score(test_y, preds_test_rf > 0.5))\n",
    "print(\"Log Loss, Test:  \", log_loss(test_y, preds_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NumTeamSeed', 'WINPCT', 'OFF_EFF', '4FACTOR', 'PIE', 'FTA', 'STL%', 'FT_RATE', 'DEF_EFF', 'PF', 'BLK%', 'Ast', 'OR', 'DenTeamElo', 'Blk', 'TOV%', 'Stl', 'TO', 'FT_PCT', 'FGA', 'NumTeamElo', 'TS%', 'FGM3', 'FTr', 'FG_PCT', 'FGM', 'DR', 'ORtg', 'eFG%', 'TRB%', 'FT/FGA', 'DenTeamSeed', 'Upset', 'AST%', 'DEF_REB_PCT', 'ASSIST_RATIO', 'TURNOVER_RATE', 'HistWinPct', 'Pace', '3PAr', 'ORB%', 'FGA3', 'FTM', 'OFF_REB_PCT']\n"
     ]
    }
   ],
   "source": [
    "variables = train_columns\n",
    "feature_importance = RF.feature_importances_\n",
    "\n",
    "feature_importance, variables = (list(t) for t in zip(*sorted(zip(feature_importance, variables))))\n",
    "\n",
    "num_features = len(feature_importance)\n",
    "features_to_keep = []\n",
    "for i in range(len(feature_importance)):\n",
    "    if feature_importance[i] > 0.0152:\n",
    "        features_to_keep.append(variables[i])\n",
    "#     print(variables[i], \":\", feature_importance[i])\n",
    "\n",
    "print(features_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.000...imators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter = VotingClassifier(estimators=[('lr', logReg), ('rf', RF)], voting='soft')\n",
    "voter.fit(train_x, train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Test:   1.0\n",
      "Log Loss, Test:   0.46147700738522024\n"
     ]
    }
   ],
   "source": [
    "preds_test = voter.predict_proba(test_x)[:,1]\n",
    "threshold = .12\n",
    "preds_test[preds_test > 1 - threshold] = 1\n",
    "preds_test[preds_test < threshold] = 0\n",
    "print(\"Accuracy, Test:  \", accuracy_score(test_y, preds_test > 0.5))\n",
    "print(\"Log Loss, Test:  \", log_loss(test_y, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss, Test:   0.40749111635437696\n"
     ]
    }
   ],
   "source": [
    "preds = (preds_test_lr + preds_test_rf) / 2\n",
    "print(\"Log Loss, Test:  \", log_loss(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
