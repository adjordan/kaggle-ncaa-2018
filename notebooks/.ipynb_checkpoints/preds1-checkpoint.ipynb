{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import math\n",
    "\n",
    "root = \"/home/austin/Github/kaggle-ncaa-2018/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant data files\n",
    "reg_ratios = pd.read_csv(root + \"derived_data/ratios/RegularSeasonDetailedResultsRatios.csv\")\n",
    "tour_ratios = pd.read_csv(root + \"derived_data/ratios/NCAATourneyDetailedResultsRatios.csv\")\n",
    "reg_season_avgs = pd.read_csv(root + \"derived_data/ratios/RegularSeasonAverageRatiosForTournamentTeams.csv\")\n",
    "\n",
    "# List of all columns in the datasets\n",
    "# all_cols = reg_ratios.columns.tolist()\n",
    "all_cols = [\"Season\", \"WTeamID\", \"WScore\", \"LTeamID\", \"LScore\", \"NumTeamWon\", \"Loc\",\n",
    "                 \"FGMR\", \"FGAR\", \"FGMR3\", \"FGAR3\", \"FTMR\", \"FTAR\", \"ORR\", \"DRR\", \"AstR\",\n",
    "                 \"TOR\", \"StlR\", \"BlkR\", \"PFR\"]\n",
    "\n",
    "# Columns that won't be used in the dataset\n",
    "cols_to_drop = [\"Season\", \"WTeamID\", \"WScore\", \"LTeamID\", \"LScore\", \"NumTeamWon\", \"Loc\"]\n",
    "\n",
    "# Columns that will be used in the dataset\n",
    "stats_columns = [item for item in all_cols if item not in cols_to_drop]\n",
    "\n",
    "# Seasons to look at\n",
    "seasons = [2011, 2012, 2013, 2014, 2015, 2016, 2017]\n",
    "\n",
    "# Specify what to use as the training data\n",
    "train_data = reg_ratios\n",
    "\n",
    "# Get all of the training data for the given years\n",
    "train_x = train_data.loc[reg_ratios[\"Season\"].isin(seasons)].drop(labels=cols_to_drop, axis=1).as_matrix()\n",
    "train_y = train_data.loc[reg_ratios[\"Season\"].isin(seasons)][\"NumTeamWon\"].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X:  (37299, 13)\n",
      "Train Y:  (37299,)\n",
      "Test X:   (469, 13)\n",
      "Test Y:   (469,)\n"
     ]
    }
   ],
   "source": [
    "tourney_games = tour_ratios.loc[tour_ratios[\"Season\"].isin(seasons)]\n",
    "tourney_games = tourney_games.reset_index(drop=True)\n",
    "\n",
    "test_x = pd.DataFrame(index=range(tourney_games.shape[0]), columns=stats_columns)\n",
    "test_y = np.ones(67*len(seasons))\n",
    "index = 0\n",
    "for i in range(len(seasons)):\n",
    "    season = seasons[i]\n",
    "    tourney_games_for_season = tourney_games.loc[tourney_games[\"Season\"] == season].reset_index()\n",
    "    for j, row in tourney_games_for_season.iterrows():\n",
    "        teamA_id = tourney_games_for_season.at[j, \"WTeamID\"]\n",
    "        teamB_id = tourney_games_for_season.at[j, \"LTeamID\"]\n",
    "        teamA_stats = reg_season_avgs.loc[(reg_season_avgs[\"TeamID\"] == teamA_id) & (reg_season_avgs[\"Season\"] == season), stats_columns].as_matrix()\n",
    "        teamB_stats = reg_season_avgs.loc[(reg_season_avgs[\"TeamID\"] == teamB_id) & (reg_season_avgs[\"Season\"] == season), stats_columns].as_matrix()\n",
    "        r = random.random()\n",
    "        if r > 0.5:\n",
    "            test_x.loc[index, stats_columns] = (teamA_stats / teamB_stats).ravel()\n",
    "        else:\n",
    "            test_x.loc[index, stats_columns] = (teamB_stats / teamA_stats).ravel()\n",
    "            test_y[index] = 0\n",
    "            \n",
    "#         if index == 0:\n",
    "#             print(test_x.loc[index, stats_columns])\n",
    "#             print(teamA_stats / teamB_stats)\n",
    "#             print(teamA_stats)\n",
    "#             print(teamB_stats)\n",
    "        index += 1\n",
    "    \n",
    "test_x = test_x.as_matrix()\n",
    "print(\"Train X: \", train_x.shape)\n",
    "print(\"Train Y: \", train_y.shape)\n",
    "print(\"Test X:  \", test_x.shape)\n",
    "print(\"Test Y:  \", test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
       "          max_features=1, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "          min_impurity_split=None, min_samples_leaf=1, min_samples_split=2,\n",
       "          min_weight_fraction_leaf=0.0, n_estimators=10000, n_jobs=-1,\n",
       "          oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ExtraTreesRegressor(n_estimators=10000, oob_score=False, max_features=1, min_samples_leaf=1, n_jobs=-1)\n",
    "clf.fit(train_x, train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Train:  1.0\n",
      "Log Loss, Train:  9.992007221626413e-16\n",
      "\n",
      "Accuracy, Test:   0.6353944562899787\n",
      "Log Loss, Test:   0.6543711896959209\n"
     ]
    }
   ],
   "source": [
    "preds_train = clf.predict(train_x)\n",
    "preds_test = clf.predict(test_x)\n",
    "\n",
    "# threshold = 0.1\n",
    "# preds_test[preds_test < threshold] = 0\n",
    "# preds_test[preds_test > 1 - threshold] = 1\n",
    "\n",
    "print(\"Accuracy, Train: \", accuracy_score(train_y, preds_train > 0.5))\n",
    "print(\"Log Loss, Train: \", log_loss(train_y, preds_train))\n",
    "print(\"\")\n",
    "print(\"Accuracy, Test:  \", accuracy_score(test_y, preds_test > 0.5))\n",
    "print(\"Log Loss, Test:  \", log_loss(test_y, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGMR : 0.19280963105162072\n",
      "FGAR : 0.0353608003037691\n",
      "FGMR3 : 0.05633005164966699\n",
      "FGAR3 : 0.03235132817765177\n",
      "FTMR : 0.10241820006118385\n",
      "FTAR : 0.08198897180906117\n",
      "ORR : 0.025395988041718148\n",
      "DRR : 0.1546563138236278\n",
      "AstR : 0.11258637108061874\n",
      "TOR : 0.05286647389268113\n",
      "StlR : 0.03902995450768735\n",
      "BlkR : 0.03732881713369233\n",
      "PFR : 0.07687709846702137\n"
     ]
    }
   ],
   "source": [
    "variables = stats_columns\n",
    "feature_importance = clf.feature_importances_\n",
    "for i in range(len(feature_importance)):\n",
    "    print(variables[i], \":\", feature_importance[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
