{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "root = \"/home/austin/Github/kaggle-ncaa-2018/\"\n",
    "\n",
    "data = pd.read_csv(root + \"derived_data/Master3.csv\")\n",
    "matchups = pd.read_csv(\"/home/austin/Github/DataFiles/TourneyWinratesBySeed.csv\")\n",
    "tour_ratios = pd.read_csv(root + \"derived_data/ratios/NCAATourneyDetailedResultsRatios.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = range(2010, 2018)\n",
    "\n",
    "all_columns = data.columns.tolist()\n",
    "non_stats_columns = [\"Season\", \"TeamID\", \"Seed\", \"Elo\", \"NumTeamID\", \"DenTeamID\"]#, \"BPI\", \"Predictor_Score\"]\n",
    "stats_columns = [c for c in all_columns if c not in non_stats_columns]\n",
    "# stats_columns_A = [\"A\"+c for c in all_columns if c not in non_stats_columns]\n",
    "# stats_columns_B = [\"B\"+c for c in all_columns if c not in non_stats_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all data in a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_games = tour_ratios.loc[tour_ratios[\"Season\"].isin(seasons)]\n",
    "tourney_games = tourney_games.reset_index(drop=True)\n",
    "extra_cols = [\"NumTeamSeed\", \"DenTeamSeed\", \"NumTeamElo\", \"DenTeamElo\", \"HistWinPct\"]\n",
    "\n",
    "pred_data = pd.DataFrame(index=range(tourney_games.shape[0]), columns= [\"Season\", \"NumTeamID\", \"DenTeamID\"] + stats_columns + extra_cols + [\"Upset\",\"NumTeamWon\"])\n",
    "pred_data.loc[:,\"NumTeamWon\"] = np.ones(tourney_games.shape[0])\n",
    "pred_data.loc[:,\"Upset\"] = np.zeros(tourney_games.shape[0])\n",
    "gap = 4\n",
    "index = 0\n",
    "for i in range(len(seasons)):\n",
    "    season = seasons[i]\n",
    "    tourney_games_for_season = tourney_games.loc[tourney_games[\"Season\"] == season].reset_index()\n",
    "    for j, row in tourney_games_for_season.iterrows():\n",
    "        pred_data.loc[index, \"Season\"] = season\n",
    "        teamA_id = tourney_games_for_season.at[j, \"WTeamID\"]\n",
    "        teamB_id = tourney_games_for_season.at[j, \"LTeamID\"]\n",
    "        \n",
    "        teamA_seed = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Seed\"]\n",
    "        teamB_seed = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Seed\"]\n",
    "        \n",
    "        teamA_elo = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Elo\"]\n",
    "        teamB_elo = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season)].reset_index(drop=True).loc[0, \"Elo\"]\n",
    "        \n",
    "        teamA_stats = data.loc[(data[\"TeamID\"] == teamA_id) & (data[\"Season\"] == season), stats_columns].as_matrix()\n",
    "        teamB_stats = data.loc[(data[\"TeamID\"] == teamB_id) & (data[\"Season\"] == season), stats_columns].as_matrix()\n",
    "        \n",
    "        r = random.random()\n",
    "        \n",
    "        if abs(teamA_seed - teamB_seed) < gap:\n",
    "            continue\n",
    "        \n",
    "        if r > 0.5:\n",
    "            pred_data.at[index, stats_columns] = (teamA_stats / teamB_stats).ravel()\n",
    "            pred_data.at[index, \"NumTeamID\"] = teamA_id\n",
    "            pred_data.at[index, \"DenTeamID\"] = teamB_id\n",
    "            pred_data.at[index, \"NumTeamSeed\"] = teamA_seed\n",
    "            pred_data.at[index, \"DenTeamSeed\"] = teamB_seed\n",
    "            pred_data.at[index, \"NumTeamElo\"] = teamA_elo\n",
    "            pred_data.at[index, \"DenTeamElo\"] = teamB_elo\n",
    "            pred_data.at[index, \"HistWinPct\"] = matchups.loc[(matchups[\"WinSeed\"] == teamA_seed) & (matchups[\"LoseSeed\"] == teamB_seed), \"1985\"].reset_index(drop=True)[0]\n",
    "            \n",
    "        else:\n",
    "            pred_data.at[index, stats_columns] = (teamB_stats / teamA_stats).ravel()\n",
    "            pred_data.at[index, \"NumTeamID\"] = teamB_id\n",
    "            pred_data.at[index, \"DenTeamID\"] = teamA_id\n",
    "            pred_data.at[index, \"NumTeamSeed\"] = teamB_seed\n",
    "            pred_data.at[index, \"DenTeamSeed\"] = teamA_seed\n",
    "            pred_data.at[index, \"NumTeamElo\"] = teamB_elo\n",
    "            pred_data.at[index, \"DenTeamElo\"] = teamA_elo\n",
    "            pred_data.at[index, \"HistWinPct\"] = matchups.loc[(matchups[\"WinSeed\"] == teamB_seed) & (matchups[\"LoseSeed\"] == teamA_seed), \"1985\"].reset_index(drop=True)[0]\n",
    "            pred_data.at[index, \"NumTeamWon\"] = 0\n",
    "            \n",
    "        if teamA_seed >= teamB_seed + gap:\n",
    "            pred_data.at[index, \"Upset\"] = 1\n",
    "        \n",
    "        index += 1\n",
    "\n",
    "pred_data = pred_data.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select which stats to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FGM', 'FGA', 'FGM3', 'FGA3', 'FTM', 'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF', 'PIE', 'FG_PCT', 'TURNOVER_RATE', 'OFF_REB_PCT', 'FT_RATE', '4FACTOR', 'OFF_EFF', 'DEF_EFF', 'ASSIST_RATIO', 'DEF_REB_PCT', 'FT_PCT', 'WINPCT', 'Pace', 'ORtg', 'FTr', '3PAr', 'TS%', 'TRB%', 'AST%', 'STL%', 'BLK%', 'eFG%', 'TOV%', 'ORB%', 'FT/FGA', 'NumTeamSeed', 'DenTeamSeed', 'NumTeamElo', 'DenTeamElo', 'HistWinPct']\n"
     ]
    }
   ],
   "source": [
    "all_train_columns = stats_columns + extra_cols\n",
    "\n",
    "# train_columns_to_drop = [\"FGA\", \"Ast\", \"FGM\", \"Blk\", \"STL%\", \"Stl\", \"DR\", \"NumTeamSeed\", \"NumTeamElo\", \"OR\", \"Pace\",\n",
    "#                          \"ASSIST_RATIO\", \"AST%\", \"PF\", \"DEF_EFF\", \"PIE\", \"FT_PCT\"]\n",
    "\n",
    "train_columns_to_drop = [\"Upset\"]\n",
    "\n",
    "train_columns = [c for c in all_train_columns if c not in train_columns_to_drop]\n",
    "print(train_columns)\n",
    "# print(pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_columns = features_to_keep\n",
    "# train_columns = ['TURNOVER_RATE', 'PF', 'WINPCT', 'DenTeamElo', 'NumTeamElo']\n",
    "# print(train_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2010, 2011, 2012, 2013, 2014, 2015, 2016]\n",
      "(154, 43)\n",
      "(154,)\n",
      "(42, 43)\n",
      "(42,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "seasons_test = [2017]\n",
    "seasons_train = [season for season in seasons if season not in seasons_test]\n",
    "print(seasons_train)\n",
    "\n",
    "# train_x = pred_data.loc[pred_data[\"Season\"].isin(seasons_train)][train_columns].as_matrix()\n",
    "# train_y = pred_data.loc[pred_data[\"Season\"].isin(seasons_train)][\"NumTeamWon\"].as_matrix()\n",
    "# test_x = pred_data.loc[pred_data[\"Season\"].isin(seasons_test)][train_columns].as_matrix()\n",
    "# test_y = pred_data.loc[pred_data[\"Season\"].isin(seasons_test)][\"NumTeamWon\"].as_matrix()\n",
    "\n",
    "pred_data_no_upset = pred_data[pred_data[\"Upset\"] == 0]\n",
    "pred_data_upset = pred_data[pred_data[\"Upset\"] == 1]\n",
    "num_upset = pred_data_upset.shape[0]\n",
    "pred_data_no_upset_sample = pred_data_no_upset.sample(num_upset)\n",
    "\n",
    "data_to_use = pd.concat([pred_data_upset, pred_data_no_upset_sample])\n",
    "data_to_use = shuffle(data_to_use)\n",
    "\n",
    "train_x = data_to_use.loc[data_to_use[\"Season\"].isin(seasons_train)][train_columns].as_matrix()\n",
    "train_y = data_to_use.loc[data_to_use[\"Season\"].isin(seasons_train)][\"NumTeamWon\"].as_matrix()\n",
    "test_x = pred_data.loc[pred_data[\"Season\"].isin(seasons_test)][train_columns].as_matrix()\n",
    "test_y = pred_data.loc[pred_data[\"Season\"].isin(seasons_test)][\"NumTeamWon\"].as_matrix()\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "\n",
    "# print(pred_data_no_upset)\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "# # train_y = to_categorical(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_x)\n",
    "# print(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75       0.65       0.61538462 0.61538462 0.63157895]\n",
      "Accuracy: 0.65 (+/- 0.10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10], 'tol': [0.0001, 0.001, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "parameters = {'C': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10],\n",
    "              'tol': [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "              }\n",
    "\n",
    "logReg = GridSearchCV(LogisticRegression(), parameters, cv=3)\n",
    "# logReg = LogisticRegression(C=10, tol=0.0001)\n",
    "\n",
    "logReg.fit(train_x, train_y.ravel())\n",
    "\n",
    "scores= cross_val_score(logReg, np.concatenate((test_x, train_x)),np.concatenate((test_y, train_y)), cv=5)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "logReg.fit(train_x, train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'tol': 0.01}\n",
      "Accuracy:  0.7142857142857143\n",
      "Log Loss, Test:   0.538190096535906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preds_train = np.array(logReg.predict_proba(train_x))\n",
    "preds_test_lr = np.array(logReg.predict_proba(test_x))[:,1]\n",
    "\n",
    "# print(preds_test)\n",
    "# print(test_y)\n",
    "\n",
    "# threshold = .1\n",
    "\n",
    "# preds_test_lr[preds_test_lr > 1 - threshold] = 1\n",
    "# preds_test_lr[preds_test_lr < threshold] = 0\n",
    "\n",
    "print(logReg.best_params_)\n",
    "\n",
    "# print(\"Accuracy, Train: \", accuracy_score(train_y, preds_train > 0.5))\n",
    "# print(\"Log Loss, Train: \", log_loss(train_y, preds_train))\n",
    "# print(\"\")\n",
    "print(\"Accuracy: \", logReg.score(test_x,test_y))\n",
    "# print(\"Accuracy, Test:  \", accuracy_score(test_y, preds_test_lr > 0.5))\n",
    "print(\"Log Loss, Test:  \", log_loss(test_y, preds_test_lr))\n",
    "print(\"\")\n",
    "# print(preds_test_lr)\n",
    "# to_print = pred_data.loc[pred_data[\"Season\"].isin(seasons_test)].reset_index(drop=True)\n",
    "\n",
    "# pd.set_option('display.max_rows', len(to_print))\n",
    "# pd.set_option('display.max_columns', to_print.shape[1])\n",
    "# print(to_print[features_to_keep])\n",
    "# pd.reset_option('display.max_rows')\n",
    "# pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65       0.5        0.56410256 0.53846154 0.57894737]\n",
      "Accuracy: 0.57 (+/- 0.10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "parameters = {'max_features': np.arange(1,12,1),\n",
    "              'min_samples_split': np.arange(2,50,4),\n",
    "              'min_samples_leaf': np.arange(1,25,4)\n",
    "              }\n",
    "\n",
    "# RF = RandomizedSearchCV(RandomForestRegressor(n_estimators=1000, n_jobs=-1, random_state=0), parameters,\n",
    "#                     cv=10, n_iter=10)\n",
    "RF = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=0)\n",
    "scores2 = cross_val_score(RF, np.concatenate((test_x, train_x)),np.concatenate((test_y, train_y)), cv=5)\n",
    "print(scores2)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores2.mean(), scores2.std() * 2))\n",
    "# clf = ExtraTreesRegressor(n_estimators=1000, max_features=1, n_jobs=-1)\n",
    "# clf = LogisticRegression(n_jobs=-1)\n",
    "RF.fit(train_x, train_y.ravel())\n",
    "\n",
    "# print(\"OOB Score: \", clf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Test:   0.6428571428571429\n",
      "Log Loss, Test:   12.33541055013504\n"
     ]
    }
   ],
   "source": [
    "# print(RF.best_params_)\n",
    "\n",
    "preds_train = RF.predict(train_x)\n",
    "# preds_test_rf = RF.predict_proba(test_x)[:,1]\n",
    "preds_test_rf = RF.predict(test_x)\n",
    "\n",
    "# threshold = .1\n",
    "# preds_test_rf[preds_test_rf > 1 - threshold] = 1\n",
    "# preds_test_rf[preds_test_rf < threshold] = 0\n",
    "\n",
    "# print(\"Accuracy, Train: \", accuracy_score(train_y, preds_train > 0.5))\n",
    "# print(\"Log Loss, Train: \", log_loss(train_y, preds_train))\n",
    "# print(\"\")\n",
    "print(\"Accuracy, Test:  \", RF.score(test_x, test_y))\n",
    "print(\"Log Loss, Test:  \", log_loss(test_y, preds_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG_PCT : 0.013731697294545733\n",
      "TO : 0.014129071307428399\n",
      "TOV% : 0.014165522015906398\n",
      "TS% : 0.014629367008433569\n",
      "OFF_EFF : 0.014858208313150111\n",
      "eFG% : 0.017268282698777053\n",
      "Blk : 0.017329545244196342\n",
      "TURNOVER_RATE : 0.018223381947596082\n",
      "FTM : 0.018446337846322984\n",
      "BLK% : 0.018732194218405784\n",
      "FTA : 0.01894987026737003\n",
      "FGA : 0.019123899415458116\n",
      "ORtg : 0.019322564647558372\n",
      "4FACTOR : 0.01940918488591067\n",
      "NumTeamSeed : 0.01957880816543807\n",
      "DenTeamSeed : 0.02065968312146631\n",
      "DR : 0.020694755503742068\n",
      "TRB% : 0.020931817610488488\n",
      "Ast : 0.021275932815187592\n",
      "FGA3 : 0.021526407765236247\n",
      "ASSIST_RATIO : 0.021607502998462005\n",
      "NumTeamElo : 0.022682083407905822\n",
      "Pace : 0.023170681565445946\n",
      "FT_RATE : 0.023411968617235414\n",
      "FT/FGA : 0.023772746011256906\n",
      "WINPCT : 0.02398216454818644\n",
      "FT_PCT : 0.024123582117236305\n",
      "PF : 0.024127325769006204\n",
      "3PAr : 0.024417963355762507\n",
      "FGM3 : 0.024607895056980893\n",
      "DEF_REB_PCT : 0.02483207320685145\n",
      "OR : 0.02492053056850508\n",
      "STL% : 0.02515522315094342\n",
      "FGM : 0.025440475293994062\n",
      "FTr : 0.02666673266038319\n",
      "Stl : 0.02714489417115981\n",
      "DEF_EFF : 0.027336566955711235\n",
      "DenTeamElo : 0.029337081947095684\n",
      "PIE : 0.029495776595560273\n",
      "AST% : 0.03458372580879477\n",
      "ORB% : 0.036453558528974715\n",
      "OFF_REB_PCT : 0.038473234490539535\n",
      "HistWinPct : 0.0512696810813899\n",
      "['TURNOVER_RATE', 'PF', 'WINPCT', 'DenTeamElo', 'NumTeamElo', 'HistWinPct']\n"
     ]
    }
   ],
   "source": [
    "variables = train_columns\n",
    "feature_importance = RF.feature_importances_\n",
    "\n",
    "feature_importance, variables = (list(t) for t in zip(*sorted(zip(feature_importance, variables))))\n",
    "\n",
    "num_features = len(feature_importance)\n",
    "# features_to_keep = []\n",
    "for i in range(len(feature_importance)):\n",
    "#     if feature_importance[i] > 0.026:\n",
    "#         features_to_keep.append(variables[i])\n",
    "    print(variables[i], \":\", feature_importance[i])\n",
    "\n",
    "print(features_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001...imators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voter = VotingClassifier(estimators=[('lr', logReg), ('rf', RF)], voting='soft')\n",
    "voter.fit(train_x, train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.7142857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "preds_test = voter.predict_proba(test_x)[:,1]\n",
    "# print(\"Accuracy, Test:  \", accuracy_score(test_y, preds_test > 0.5))\n",
    "# print(\"Log Loss, Test:  \", log_loss(test_y, preds_test))\n",
    "print(\"Accuracy \", voter.score(test_x,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7        0.57142857 0.51020408 0.5625    ]\n",
      "Accuracy: 0.59 (+/- 0.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "RF = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=0)\n",
    "logReg = LogisticRegression(C=1, tol=0.01)\n",
    "voter = VotingClassifier(estimators=[('lr', logReg), ('rf', RF)], voting='soft')\n",
    "scores2 = cross_val_score(voter, np.concatenate((test_x, train_x)),np.concatenate((test_y, train_y)), cv=4)\n",
    "print(scores2)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores2.mean(), scores2.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
