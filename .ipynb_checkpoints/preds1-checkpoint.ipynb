{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ratios = pd.read_csv(\"RegularSeasonDetailedResultsRatios.csv\")\n",
    "cols_to_drop = [\"Season\", \"DayNum\", \"WTeamID\", \"WScore\", \"LTeamID\", \"LScore\", \"NumTeamWon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all rows for 2017 season and drop irrelevant variables\n",
    "test_data = reg_ratios.loc[reg_ratios['Season'] == 2017]\n",
    "test_x = test_data.drop(columns=cols_to_drop, axis=1).as_matrix()\n",
    "test_y = test_data[\"NumTeamWon\"].as_matrix()\n",
    "\n",
    "# Get rows for all other seasons, drop irrelevant variables\n",
    "train_data = reg_ratios[reg_ratios.Season != 2017]\n",
    "train_x = train_data.drop(labels=cols_to_drop, axis=1).as_matrix()\n",
    "train_y = train_data[\"NumTeamWon\"].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "clf = RandomForestRegressor(n_estimators=100)\n",
    "clf.fit(train_x, train_y.ravel())\n",
    "# print(\"Out-of-bag score: {}\".format(clf.oob_score_))\n",
    "# acc = clf.score(test_x, test_y.ravel())\n",
    "# print(\"Test accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "preds = clf.predict(test_x)\n",
    "print(preds.shape)\n",
    "print(test_y.shape)\n",
    "print(\"Accuracy score\", accuracy_score(test_y, preds > 0.5))\n",
    "print(\"Log loss\", log_loss(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New model:\n",
    "# Train on all regular season data, including teamID\n",
    "# Predict based on team's last X games\n",
    "games_to_use = 20\n",
    "import math\n",
    "\n",
    "reg_ratios = pd.read_csv(\"RegularSeasonDetailedResultsRatios.csv\")\n",
    "tour_ratios = pd.read_csv(\"NCAATourneyDetailedResultsRatios.csv\")\n",
    "all_cols = [\"Season\", \"DayNum\", \"WTeamID\", \"WScore\", \"LTeamID\", \"LScore\", \"NumTeamWon\", \"Loc\",\n",
    "                 \"FGMR\", \"FGAR\", \"FGMR3\", \"FGAR3\", \"FTMR\", \"FTAR\", \"ORR\", \"DRR\", \"AstR\",\n",
    "                 \"TOR\", \"StlR\", \"BlkR\", \"PFR\"]\n",
    "\n",
    "# Add day numbers to regular season games\n",
    "regular_season_data = pd.read_csv(\"RegularSeasonDetailedResults.csv\")\n",
    "reg_ratios[\"DayNum\"] = regular_season_data[\"DayNum\"]\n",
    "reg_ratios[\"Season\"] = reg_ratios[\"Season\"].astype(int)\n",
    "# print(reg_ratios.loc[reg_ratios[\"Season\"] == 2017])\n",
    "# print(reg_ratios)\n",
    "\n",
    "# Get train data: all regular season games\n",
    "# cols_to_drop = [\"Season\", \"DayNum\", \"WScore\", \"WTeamID\", \"LScore\", \"LTeamID\", \"NumTeamWon\", \"Loc\"]\n",
    "cols_to_drop = [\"Season\", \"DayNum\", \"WScore\", \"LScore\", \"NumTeamWon\", \"Loc\"]\n",
    "\n",
    "train_data = reg_ratios\n",
    "train_x = reg_ratios.loc[reg_ratios[\"Season\"] == 2017].drop(labels=cols_to_drop, axis=1).as_matrix()\n",
    "train_y = reg_ratios.loc[reg_ratios[\"Season\"] == 2017][\"NumTeamWon\"].as_matrix()\n",
    "\n",
    "# Get all team IDs from the tournament\n",
    "tour_avg_cols = [\"TeamID\", \"FGMR\", \"FGAR\", \"FGMR3\", \"FGAR3\", \"FTMR\", \"FTAR\", \"ORR\", \"DRR\", \"AstR\",\n",
    "                 \"TOR\", \"StlR\", \"BlkR\", \"PFR\"]\n",
    "team_ids = pd.unique(tour_ratios.loc[tour_ratios[\"Season\"] == 2017, [\"WTeamID\", \"LTeamID\"]].values.ravel('K'))\n",
    "tour_avgs = pd.DataFrame(index=range(team_ids.shape[0]), columns=tour_avg_cols)\n",
    "tour_avgs.loc[:,\"TeamID\"] = team_ids\n",
    "# print(test_x)\n",
    "\n",
    "for team_id in team_ids:\n",
    "    games_all = reg_ratios.loc[(reg_ratios[\"WTeamID\"] == team_id) | (reg_ratios[\"LTeamID\"] == team_id)]\n",
    "    games = games_all.loc[games_all[\"Season\"] == 2017]\n",
    "    games.reset_index(inplace=True, drop=True)\n",
    "    games_stats = games.loc[:, \"FGMR\":\"PFR\"]\n",
    "    for i, row in games.iterrows():\n",
    "        if ((team_id == row[\"WTeamID\"]) & (row[\"NumTeamWon\"] == 0)) | ((team_id == row[\"LTeamID\"]) & (row[\"NumTeamWon\"] == 1)):\n",
    "            games_stats.iloc[i] = games_stats.iloc[i].apply(np.reciprocal)\n",
    "#             if math.isinf(games_stats.at[i,\"Loc\"]):\n",
    "#                 games_stats.loc[i,\"Loc\"] = 0\n",
    "    \n",
    "    tour_avgs.loc[tour_avgs[\"TeamID\"] == team_id, \"FGMR\":\"PFR\"] = games_stats.mean(axis=0).ravel()\n",
    "    \n",
    "tour_avgs = tour_avgs.set_index(\"TeamID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test_x_columns = [item for item in all_cols if item not in cols_to_drop]\n",
    "# print(test_x_columns)\n",
    "\n",
    "tourney_games_2017 = tour_ratios.loc[tour_ratios[\"Season\"] == 2017]\n",
    "tourney_games_2017 = tourney_games_2017.reset_index(drop=True)\n",
    "# print(tourney_games_2017.shape)\n",
    "test_x = pd.DataFrame(index=range(tourney_games_2017.shape[0]), columns=test_x_columns)\n",
    "test_y = np.ones(67)\n",
    "for i, row in tourney_games_2017.iterrows():\n",
    "    teamA_id = tourney_games_2017.at[i, \"WTeamID\"]\n",
    "    teamB_id = tourney_games_2017.at[i, \"LTeamID\"]\n",
    "    teamA_stats = tour_avgs.loc[teamA_id, \"FGMR\":\"PFR\"].as_matrix()\n",
    "    teamB_stats = tour_avgs.loc[teamB_id, \"FGMR\":\"PFR\"].as_matrix()\n",
    "    r = random.random()\n",
    "    if r > 0.5:\n",
    "        test_x[\"WTeamID\"] = teamA_id\n",
    "        test_x[\"LTeamID\"] = teamB_id\n",
    "#         test_x[\"Loc\"] = 0\n",
    "        test_x.loc[i, \"FGMR\":\"PFR\"] = teamA_stats / teamB_stats\n",
    "    else:\n",
    "        test_x[\"WTeamID\"] = teamB_id\n",
    "        test_x[\"LTeamID\"] = teamA_id\n",
    "#         test_x[\"Loc\"] = 0\n",
    "        test_x.loc[i, \"FGMR\":\"PFR\"] = teamB_stats / teamA_stats\n",
    "        test_y[i] = 0\n",
    "    \n",
    "# print(test_x)\n",
    "test_x = test_x.as_matrix()\n",
    "# print(train_x.shape)\n",
    "# print(train_y.shape)\n",
    "# print(test_x.shape)\n",
    "# print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oob score: {} 0.5939268001571109\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# num_splits = 10\n",
    "# ll_best = 1000\n",
    "# num_features_best = 1\n",
    "# kf = KFold(n_splits=num_splits)\n",
    "# for num_features_to_use in range(1, 17):\n",
    "#     ll_feature_count = 0\n",
    "#     for train, test in kf.split(train_x):\n",
    "#         clf2 = RandomForestRegressor(n_estimators=5000, oob_score=True, max_features=num_features_to_use)\n",
    "#         clf2.fit(train_x[train], train_y[train].ravel())\n",
    "#         preds = clf2.predict(train_x[test])\n",
    "#         ll_feature_count = log_loss(train_y[test], preds) / num_splits\n",
    "    \n",
    "#     if ll_feature_count < ll_best:\n",
    "#         ll_best = ll_feature_count\n",
    "#         num_features_best = num_features_to_use\n",
    "\n",
    "# print(\"Best number of features to use: {}\", num_features_best)\n",
    "# print(\"Log loss achieved: {}\", ll_best)\n",
    "\n",
    "clf2 = RandomForestRegressor(n_estimators=5000, oob_score=True, max_features=1, min_samples_leaf=50, n_jobs=-1)\n",
    "clf2.fit(train_x, train_y.ravel())\n",
    "print(\"oob score: {}\", clf2.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.6567164179104478\n",
      "[0.5529672  0.30315836 0.19299759 0.66537479 0.26698175 0.58109421\n",
      " 0.32790715 0.75108253 0.0873931  0.38975715 0.55257351 0.4925039\n",
      " 0.55149249 0.35742411 0.16704249 0.14352955 0.29118399 0.61488758\n",
      " 0.77630509 0.52660116 0.35514744 0.54790598 0.7987679  0.19826557\n",
      " 0.25854854 0.83159468 0.22918285 0.71339927 0.32459922 0.68709596\n",
      " 0.10688774 0.47425838 0.78696425 0.79150459 0.74699067 0.80478267\n",
      " 0.30710608 0.58659479 0.50231626 0.92654705 0.19745819 0.62252687\n",
      " 0.35327676 0.29785228 0.59675443 0.76506399 0.27246753 0.48939752\n",
      " 0.79635179 0.2119273  0.44784822 0.38871168 0.15475425 0.2404387\n",
      " 0.63272652 0.15888957 0.35670231 0.50404244 0.24122572 0.56646692\n",
      " 0.89721805 0.72741461 0.4919409  0.33731644 0.84516809 0.47057346\n",
      " 0.16840658]\n",
      "[1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
      "Log loss 0.599546971489871\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds = clf2.predict(test_x)\n",
    "print(\"Accuracy score\", accuracy_score(test_y, preds > 0.5))\n",
    "print(preds)\n",
    "print(test_y)\n",
    "print(\"Log loss\", log_loss(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "12\n",
      "0.013204296094581802\n"
     ]
    }
   ],
   "source": [
    "print(num_features_to_use)\n",
    "print(num_features_best)\n",
    "print(ll_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
